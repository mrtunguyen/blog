{
  
    
        "post0": {
            "title": "Autoregressive Generative Models",
            "content": "Introduction . Generative model is a subset of unsupervised learning which has been recieving a lot of attention for last few years. The idea is that given a training dataset, we will use models or algorithms to generate new samples with the same distribution. . “What I cannot create, I do not understand.” —Richard Feynman . Suppose we have a dataset containing images of dogs. We may wish to build a model that can generate a new image of a dog that has never existed but still looks real because the model has learned the general rules that govern the appearance of a dog. This is the kind of problem that can be solved using generative modeling. . In mathematical terms, generative modeling estimates $p(x)$ —the probability of observing observation $x$. In fact, our model tries to learn to construct an estimate $p_{model}(x)$ as similar as possible to the probability density function $p_{data}(x)$. . In this blog, we will take a deep look at one of popular approaches to tackle this problem which is Autoregressive Generative Models . Autoregressive Generative Model . The idea behind this type of model is that if we consider all obversations as a sequence, the probability of observing one data point depends only on the previous ones but not the ones after it, we can compute $p(x)$ as a product of conditional distributions by applying product rule). In a nutshel, autoregressive models define the joint distribution using conditionals over each feature, given the values of the previous features. . $$ mathbf{p}( mathbf{x}) = prod_{i=1}^{N} mathbf{x}_i = prod_{i=1}^{N} mathbf{p} big( mathbf{x}_i | mathbf{x}_1, mathbf{x}_2, ..., mathbf{x}_{i-1} big) tag{1}$$ . For example, we can use autoregressive generative models to predict the output at timestep t given its previous timesteps in text-to-speech problem. Or the probability of a pixel from an image to have a specific intensity value is conditioned by the values of all previous pixels; and the probability of an image (the joint distribution of all pixels) is the combination of the probability of all its pixels. . Masked Autoencoder for Distribution Estimation (MADE) . The idea of MADE is built on top of autoencoder architecture. So, we&#39;ll have a quick look on vanila autoencoder. . Autoencoder . Our primary goal is take an input sample $ mathbf{x}$ and transform it to some latent dimension $ mathbf{z}$ (encoder), which hopefully is a good representation of the original data. . what is a good representation? &quot;A good representation is one where you can reconstruct the original input!&quot;. . The process of transforming the latent dimension $ mathbf{z}$ back to a reconstructed version of the input $ mathbf{ hat{x}}$ is called the decoder. It&#39;s an autoencoder because it&#39;s using the same value $ mathbf{x}$ value on the input and output. . Mathematically, an encoder is mapping an input $ mathbf{x}$ to a feature vector $ mathbf{h} = mathbf{f}_{ theta}( mathbf{x})$ while a decoder tries to map from feature space back into input space producing a reconstruction $ mathbf{ hat{x}}= mathbf{g}_{ theta}( mathbf{h}) = mathbf{g}_{ theta} big( mathbf{f}_{ theta}( mathbf{x}) big)$. The set of parameters $ theta$ of the encoder and decoder are learned simultaneously on the task of reconstructing as well as possible the original input, i.e.attempting to incur the lowest possible reconstruction error $ mathcal{L}( mathbf{x}, mathbf{ hat{x}})$ - a measure of the discrepancy between $ mathbf{x}$ and its reconstruction $ mathbf{ hat{x}}$ - over training examples . . To train autoencoder, we use cross-entropy loss: begin{align*} mathcal{L_{ text{binary}}}({ bf mathbf{x}}) &amp;= sum_{i=1}^N - mathbf{x}_i log hat{ mathbf{x}}_i - (1- mathbf{x}_i) log(1- hat{ mathbf{x}_i}) tag{2} end{align*} . To capture the structure of the data-generating distribution, it is therefore important that something in the training criterion or the parametrization prevents the auto-encoder from learning the identity function, which has zero reconstruction error everywhere. This is achieved through various means regularized autoencoders (refer to [3] to see more about it) . Masked Autoencoders . Since autoencoder is to reconstruct the input from learning a latent representation of data in an unsupervised manner, it can&#39;t provide a proper probability distribution. Therefore, it can usually be used in applications such as denoising images but can&#39;t generate a total new sample. . The reason is that in autoencoder, each output $ mathbf{ hat{x}}_i$ could depend on any of the components input $ mathbf{x}_1,…, mathbf{x}_n$. So in order to convert to our autoregressive models as defined above, we can modify the structure so that $ mathbf{ hat{x}}_i$ only depend on previous components $ mathbf{x}_1,…, mathbf{x}_{i-1}$ but not the future ones $ mathbf{x}_{i+1},…, mathbf{x}_n$. . The principle becomes as following: . Each output of the network $ mathbf{ hat{x}}_i$ represents the probability distribution $ mathbf{p} big( mathbf{x}_i | mathbf{x}_{&lt;i} big)$ . | Each output $ mathbf{ hat{x}}_i$ can only have connections (recursively) to smaller indexed inputs $ mathbf{x}_{&lt;i}$ and not any of the other ones. . | . To persuit this principle, the MADE authors came up with the idea of masked autoencoders. Since output $ mathbf{ hat{x}}_i$ must depend only on the preceding inputs $ mathbf{x}_{&lt;i}$, it means that there must be no computational path between output unit $ mathbf{ hat{x}}_i$ and any of the input units $ mathbf{x}_i, ... mathbf{x}_N$. To do so, we will zero-out the weights we don&#39;t want by creating a binary mask matrix, whose entries that are set to 0 correspond to the connections we wish to remove. . Take a simple case when encoder and decoder are only one layer of feed-forward. . begin{align*} { bf h}({ bf x}) &amp;= { bf g}({ bf b} + { bf Wx}) { hat{ bf x}} &amp;= text{sigm}({ bf c} + { bf V h(x)}) end{align*}where . $ odot$ is an element wise product . | ${ bf x}, hat{ bf x}$ is our vectors of input/output respectively . | $ bf h(x)$ is the hidden layer . | $ bf g(⋅)$ is the activation function of the hidden layer . | $ text{sigm}$ is the sigmoid activation function of the output layer . | $ bf b$, $ bf c$ are the constant biases for the hidden/output layer respectively . | $ bf W$, $ bf V$ are the weight matrices for the hidden/output layer respectively . | . Denote $ bf M^W$, $ bf M^V$ the masks for $ bf W$ and $ bf V$ respectively. The equations with masked autoencoders become: . $$ begin{align*} { bf h}({ bf x}) &amp;= { bf g}({ bf b} + { bf (W odot M^W)x}) { hat{ bf x}} &amp;= text{sigm}({ bf c} + { bf (V odot M^V)h(x)}) end{align*} $$The last problem is only find a way to construct masks $ bf M^W$, $ bf M^V$ which sastify autoregressive property. Let $m^{l}(k)$ be the index assigned to hidden node $k$ in layer $l$. The condition would be as follows: . First, for each hidden layer $l$, we sample $m^{l}(k)$ from a uniform distribution with range $[1,D−1]$. The index $D$ should be never used because nothing should depend on $D^{th}$ input | For a given node, it only connects to nodes in the previous layer that have an index less than or equal to its index. | . $$ M^{W^l}_{k&#39;, k} = begin{cases} 1 text{ if } m^l(k&#39;) geq m^{l-1}(k) 0 text{ otherwise} end{cases} $$ The output mask is slightly different: | . $$ M^{V}_{d, k} = begin{cases} 1 text{ if } d &gt; m^{L}(k) 0 text{ otherwise} end{cases} $$ . In figure 2: . output 1 is not connected to anything. It will just be estimated with a single constant parameter derived from the bias node. Otherwise, output 2 is only connected to hiddens which are only connected to input 1. Finally, output 3 is connected to hiddens which come from input 1 and input 2 . | On the other hand, input 3 is connected to nothing because no node should depend on it (autoregressive property). . | . Discussion . Does ordering of input matters? | Actually, there is no natural ordering input. We can shuffle the input dimensions, so that MADE is able to model any arbitrary ordering. . How can we generate new samples? | Sampling steps: . Randomly generate vector x, set $i=1$ | Feed $ bf x$ into autoencoder and generate outputs $ hat{ bf x}$ for the network, set $p = bf hat{x}_i$ | Sample from a Bernoulli distribution with parameter p, set input ${ bf x}_i=Bernoulli(p)$ | Increment $i$ and repeat steps 2-4 until $i &gt; D$. | . Implementation . The Pytorch code implementation of MADE is borrowed from this repo. &gt; In this example, We will build a network training on binarized MNIST dataset . #collapse-hide from torchvision import transforms from torchvision.datasets import MNIST from torchvision.utils import make_grid import torch import torch.utils.data as data from torch import optim from torch import nn import torch.nn.functional as F import numpy as np import matplotlib.pyplot as plt %matplotlib inline def load_data(): transform = transforms.Compose([ transforms.ToTensor(), lambda x: (x &gt; 0.5).float() #binarize image ]) train_dset = MNIST(&#39;data&#39;, transform=transform, train=True, download=True) test_dset = MNIST(&#39;data&#39;, transform=transform, train=False, download=True) train_loader = data.DataLoader(train_dset, batch_size=128, shuffle=True, pin_memory=True, num_workers=2) test_loader = data.DataLoader(test_dset, batch_size=128, shuffle=True, pin_memory=True, num_workers=2) return train_loader, test_loader def plot_train_curves(epochs, train_losses, test_losses, title=&#39;&#39;): x = np.linspace(0, epochs, len(train_losses)) plt.figure() plt.plot(x, train_losses, label=&#39;train_loss&#39;) if test_losses: plt.plot(x, test_losses, label=&#39;test_loss&#39;) plt.xlabel(&#39;Epoch&#39;) plt.ylabel(&#39;Loss&#39;) plt.title(title) plt.legend() plt.show() def visualize_batch(batch_tensor, nrow=8, title=&#39;&#39;, figsize=None): grid_img = make_grid(batch_tensor, nrow=nrow) plt.figure(figsize=figsize) plt.title(title) plt.imshow(grid_img.permute(1, 2, 0)) plt.axis(&#39;off&#39;) plt.show() train_loader, test_loader = load_data() . . #collapse-hide def train(model, train_loader, optimizer): model.train() for x, _ in train_loader: loss = model.nll(x) optimizer.zero_grad() loss.backward() optimizer.step() return model def eval_loss(model, data_loader): model.eval() total_loss = 0 with torch.no_grad(): for x, _ in data_loader: loss = model.nll(x) total_loss += loss * x.shape[0] avg_loss = total_loss / len(data_loader.dataset) return avg_loss.item() def train_epochs(model, train_loader, test_loader, train_args): epochs, lr = train_args[&#39;epochs&#39;], train_args[&#39;lr&#39;] optimizer = optim.Adam(model.parameters(), lr=lr) train_losses, test_losses = [], [] samples = model.sample(64) visualize_batch(samples, title=f&#39;Intialization&#39;) for epoch in range(epochs): model.train() model = train(model, train_loader, optimizer) train_loss = eval_loss(model, train_loader) train_losses.append(train_loss) if test_loader is not None: test_loss = eval_loss(model, test_loader) test_losses.append(test_loss) samples = model.sample(64) if epoch % 10 == 0: print(f&#39;Epoch {epoch} Test Loss: {test_losses[epoch] / np.log(2):.4f} bits/dim&#39;) visualize_batch(samples, title=f&#39;Epoch {epoch}&#39;) if test_loader is not None: print(&#39;Test Loss&#39;, test_loss) plot_train_curves(epochs, train_losses, test_losses, title=&#39;Training Curve&#39;) . . class MaskedLinear(nn.Linear): &quot;&quot;&quot; same as Linear except has a configurable mask on the weights &quot;&quot;&quot; def __init__(self, in_features, out_features, bias=True): super().__init__(in_features, out_features, bias) self.register_buffer(&#39;mask&#39;, torch.ones(out_features, in_features)) def set_mask(self, mask): self.mask.data.copy_(torch.from_numpy(mask.astype(np.uint8).T)) def forward(self, input): return F.linear(input, self.mask * self.weight, self.bias) class MADE(nn.Module): def __init__(self, device): super().__init__() self.nin = 784 #28 * 28 self.nout = 784 self.hidden_sizes = [512, 512, 512] self.device = device # define a simple MLP neural net self.net = [] hs = [self.nin] + self.hidden_sizes + [self.nout] for h0, h1 in zip(hs, hs[1:]): self.net.extend([ MaskedLinear(h0, h1), nn.ReLU(), ]) self.net.pop() # pop the last ReLU for the output layer self.net = nn.Sequential(*self.net).to(device) self.m = {} self.create_mask() # builds the initial self.m connectivity def create_mask(self): L = len(self.hidden_sizes) # sample uniform distribution the order of the inputs and the connectivity of all neurons self.m[-1] = np.arange(self.nin) for l in range(L): self.m[l] = np.random.randint(self.m[l - 1].min(), self.nin - 1, size=self.hidden_sizes[l]) # construct the mask matrices masks = [self.m[l - 1][:, None] &lt;= self.m[l][None, :] for l in range(L)] masks.append(self.m[L - 1][:, None] &lt; self.m[-1][None, :]) # set the masks in all MaskedLinear layers layers = [l for l in self.net.modules() if isinstance(l, MaskedLinear)] for l, m in zip(layers, masks): l.set_mask(m) def nll(self, x): x = x.view(-1, 784).to(self.device) # Flatten image logits = self.net(x) return F.binary_cross_entropy_with_logits(logits, x) def sample(self, n): samples = torch.zeros(n, 784).to(self.device) with torch.no_grad(): for i in range(784): logits = self.net(samples)[:, i] probs = torch.sigmoid(logits) samples[:, i] = torch.bernoulli(probs) samples = samples.view(n, 1, 28, 28) return samples.cpu() . train_args = {&#39;epochs&#39;: 20, &#39;lr&#39;: 0.01} device = &#39;cuda&#39; model = MADE(device) . train_epochs(model, train_loader, test_loader, train_args) . Epoch 0 Test Loss: 0.2613 bits/dim . Epoch 5 Test Loss: 0.2191 bits/dim . Epoch 10 Test Loss: 0.2124 bits/dim . Epoch 15 Test Loss: 0.2096 bits/dim . Test Loss 0.14515839517116547 . PixelCNN . PixelCNN is a deep autoregressive generative model for images. Let&#39;s consider an image of size $n×n$, each pixel in image is a data point $ bf x= big big }$. The model starts generating pixels from the top left corner, from left to right and top to bottom (raster scanning). . . Each pixel $ bf{x}_i$ is in turn jointly determined by three values,one for each of the color channels Red, Green and Blue (RGB). Each of the colors is thus conditioned on the other channels as well as on all the previously generated pixels. $$ mathbf p( mathbf x_i| mathbf x_{&lt;i}) = mathbf p( mathbf x_{i,R}| mathbf x_{&lt;i}). mathbf p(x_{i,G}| mathbf x_{&lt;i}, mathbf x_{i,R}). mathbf p(x_{i,B}| mathbf x_{&lt;i}, mathbf x_{i,R}, mathbf x_{i,G})$$ . Note: Along with PixelCNN, the paper authors also proposed PixelRNN with the same analogy as PixelCNN. However, PixelRNN with sequential dependency between LSTM states is very expensive for the computation. So this method will not be detailed in this blog. Check the paper if you are interested in it. . Masked spatial convolution . The masked use the convolution filter to slide over image which multiplies each element and sums them together to produce a single response. However, we cannot use this filter because a generated pixel should not know the intensities of future pixel values. To counter this issue, we use a mask on top of the filter to only choose prior pixels and zeroing the future pixels to negate them from calculation. . . Blind spot . PixelCNN masking has one problem: blind spot in receptive field because the capturing of receptive field by a CNN proceed in a triangular fashion. . . In order to address the blind spot, the authors use two filters (horizontal and vertical stacks) in conjunction to allow for capturing the whole receptive ﬁeld. . Vertical stack: conditions on all the pixels in the rows above the current pixel. It doesn&#39;t have any masking, allow the receptive field to grow in a rectangular fashion without any blind spot | Horizontal stack: conditions on the current row and takes as input the output of previous layer as well as of the vertical stack. | . . Gated PixelCNN . The PixelCNN only takes into consideration the neighborhood region and the depth of the convolution layers to make its predictions. To improve the performance of PixelCNN, the authors replaced the rectified linear units between the masked convolutions with the following gated activation function in order to model more complex interactions: $$ mathbf{y} = tanh ( mathbf W_{k,f} ast mathbf{x}) odot sigma ( mathbf W_{k,g} ast mathbf{x})$$ . where: $*$ is the convolutional operator. $ odot$ is the element-wise product. $ sigma$ is the sigmoid non-linearity $k$ is the number of the layer $tanh(W_{k,f} ast mathbf x)$ is a classical convolution with tanh activation function. $ sigma(W_{k,g} ast mathbf x)$ are the gate values (0 = gate closed, 1 = gate open). $W_{k,f}$ and $W_{k,g}$ are learned weights. $f, g$ are the different feature maps . A gated block is represented in Figure 6. There are 2 things to notice here: . the vertical stack contributes to the horizontal stack with the $1 times1$ convolution while vertical stack should not access any information horizontal stack has - otherwise it will have access to pixels it shouldn’t see. However, the vertical stack can be vertically connected as it predicts pixel following those in the vertical stack. | The convolutions with $W_f$ and $W_g$ are not combined into a single operation (which is essentially the masked convolution) to increase parallelization. The parallelization splits the $2p$ features maps into two groups of $p$ | . . Conditional PixelCNN . Sometimes we want to integrate some high-level information before feed the network, for example provising an image to the network with the associated classes in CIFAR datasets. During training we feed image as well as class to our network to make sure network would learn to incorporate that information as well. During inference we can specify what class our output image should belong to. . For a conditional PixelCNN, we represent a provided high-level image description as a latent vector $ mathbf h$, wherein the purpose of the latent vector is to model the conditional distribution $p( mathbf{x} vert mathbf{h})$ such that we get a probability as to if the images suites this description. The conditional PixelCNN models based on the following distribution: $$p( mathbf{x} vert mathbf{h}) = prod_{i=1}^{n^2} p(x_i vert x_1, cdots, x_{i-1}, mathbf{h})$$ . Add terms $ mathbf h$ before the non-linearities: . $$ mathbf{y} = tanh (W_{k,f} ast mathbf{x} {+ V_{k,f}^ top mathbf{h}} ) odot sigma (W_{k,g} ast mathbf{x} {+ V_{k,g}^ top mathbf{h} })$$ . If the latent vector $ mathbf h$ is a one-hot encoding vector that provides the class labels, which is equivalent to the adding a class dependent bias at every layer. So, the conditioning is dependent on “what should the image contain” rather than the location of contents in the image. | To add the location dependency to the model, we use a transposed convolution to map $ mathbf h$ to a spatial representation $s=deconv( mathbf h)$ to produce the output $ mathbf s$ of the same shape as the image: | . $$ mathbf{y} = tanh (W_{k,f} ast mathbf{x} {+ V_{k,f} ast mathbf{s}} ) odot sigma (W_{k,g} ast mathbf{x} {+ V_{k,g} ast mathbf{s} })$$ . Implementation . The Pytorch code implementation of Gated PixelCNN is borrowed from this repo. . class HoriVertStackConv2d(nn.Module): def __init__(self, mask_type, in_channels, out_channels, k=3, padding=1): super().__init__() self.vertical = nn.Conv2d(in_channels, out_channels, kernel_size=k, padding=padding, bias=False) self.horizontal = nn.Conv2d(in_channels, out_channels, kernel_size=(1, k), padding=(0, padding), bias=False) self.vtohori = nn.Conv2d(out_channels, out_channels, kernel_size=1, bias=False) self.register_buffer(&#39;vmask&#39;, self.vertical.weight.data.clone()) self.register_buffer(&#39;hmask&#39;, self.horizontal.weight.data.clone()) self.vmask.fill_(1) self.hmask.fill_(1) # zero the bottom half rows of the vmask self.vmask[:, :, k // 2 + 1:, :] = 0 # zero the right half of the hmask self.hmask[:, :, :, k // 2 + 1:] = 0 if mask_type == &#39;A&#39;: self.hmask[:, :, :, k // 2] = 0 def down_shift(self, x): x = x[:, :, :-1, :] pad = nn.ZeroPad2d((0, 0, 1, 0)) return pad(x) def forward(self, x): vx, hx = x.chunk(2, dim=1) self.vertical.weight.data *= self.vmask self.horizontal.weight.data *= self.hmask vx = self.vertical(vx) hx = self.horizontal(hx) # Allow horizontal stack to see information from vertical stack hx = hx + self.vtohori(self.down_shift(vx)) return torch.cat((vx, hx), dim=1) # PixelCNN using horizontal and vertical stacks to fix blind-spot class HoriVertStackPixelCNN(nn.Module): name = &#39;HoriVertStackPixelCNN&#39; def __init__(self, n_layers, device): super().__init__() model = [HoriVertStackConv2d(&#39;A&#39;, 1, 1, 3, padding=1)] for _ in range(n_layers - 2): model.extend([HoriVertStackConv2d(&#39;B&#39;, 1, 1, 3, padding=1)]) model.append(HoriVertStackConv2d(&#39;B&#39;, 1, 1, 3, padding=1)) self.net = nn.Sequential(*model).to(device) self.device = device def forward(self, x): x = x.to(self.device) return self.net(torch.cat((x, x), dim=1)).chunk(2, dim=1)[1] def nll(self, x): x = x.to(self.device) logits = self(x) return F.binary_cross_entropy_with_logits(logits, x) def sample(self, n): samples = torch.zeros(n, 1, 28, 28) #here we sample only one channel instead of three for simplicity with torch.no_grad(): for r in range(28): for c in range(28): logits = self(samples)[:, :, r, c] probs = torch.sigmoid(logits) samples[:, :, r, c] = torch.bernoulli(probs) return samples.cpu() . train_args = {&#39;epochs&#39;: 41, &#39;lr&#39;: 0.0002} device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; model = HoriVertStackPixelCNN(4, device) . train_epochs(model, train_loader, test_loader, train_args) . Epoch 0 Test Loss: 0.9882 bits/dim . Epoch 10 Test Loss: 0.7984 bits/dim . Epoch 20 Test Loss: 0.7677 bits/dim . Epoch 30 Test Loss: 0.7616 bits/dim . Epoch 40 Test Loss: 0.7589 bits/dim . Test Loss 0.5260247588157654 . PixelCNN++ . PixelCNN typically consists of a stack of masked convolutional layers that takes an $N times N times 3$ image as input and produces $N times N times 3 times 256$ (probability of pixel intensity) predictions as output. The softmax layer in PixelCNN to compute the conditional distribution of a sub-pixel is a full of 256-softmax. There are 2 issues with this approache: First, it is very costly in terms of memory. Second, it is missing the continuity property data. It means that the model does not know that a value of 128 is close to a value of 127 or 129 but no different than 255 for instance. . To address this issue, the paper authors came up with the new model including following modifications compared to PixelCNN: . Discretized logistic mixture likelihood . For each sub-pixel, generate a continuous distribution $ν$ representing the colour intensity instead of discrete distribution. For example, ν could be a mixture of logistic distribution parameterized by $ mu,s$ and the mixture weights $ pi$. $$ nu sim sum_{i=1}^K pi_i logistic( mu_i, s_i) $$ | We then convert this intensity to a mass function by assigning regions of it to the 0 to 255 pixels: | . $$ P(x| mu,s) = begin{cases} sigma( frac{x- mu+0.5}{s}) &amp; text{for } x = 0 sigma( frac{x- mu+0.5}{s}) - sigma( frac{x- mu-0.5}{s}) &amp; text{for } 0 &lt; x &lt; 255 1 - sigma( frac{x- mu-0.5}{s}) &amp; text{for } x = 255 end{cases} $$where $ sigma$ is the sigmoid function. . Conditioning on whole pixels . PixelCNN factorizes the model over the 3 sub pixels according to the color(RGB) which however, complicates the model. The dependency between color channels of a pixel is relatively simple and doesn’t require a deep model to train. Therefore, it is better to condition on whole pixels instead of separate colors and then output joint distributions over all 3 channels of the predicted pixel. . We first predict the red channel using a discretized mixture of logistic | Next, we predict the green channel using a predictive distribution of the same form. Here we allow the means of the mixture components to linearly depend on the value of the red sub-pixel. | Finally, we model the blue channel in the same way, where we again only allow linear dependency on the red and green channels. | . Downsampling versus dilated convolution . The PixelCNN uses convolutions with small receptive field which is good at capturing local dependencies, but not necessarily at modeling long range structure. To overcome this, we downsample the layers by using convolutions of stride 2. Downsampling reduces input size and thus improves relative size of receptive field which leads to some loss of information but it can be compensated by adding extra short-cut connections. . Adding short-cut connections . The idea is pretty the same as Unet model by introducing additional short-cut connections into the model to recover the losed informations from lower layers to higher layers of the model. . Regularization using dropout The PixelCNN model is powerful enough to overfit on training data, leads to lower perceptual quality of images while generating data. One effective way of regularizing neural networks is dropout (Srivas-tava et al., 2014). . Reference . [1]. Deep Unsupervised Learning course -- UC Berkeley [2]. Brian Keng Autoregressive autoencoder [3]. Bengio et al. 2014 Representation Learning: A Review and NewPerspectives [4] wiki notes- Conditional Image Generation with PixelCNN Decoders [5]. Van den Oord et al. 2016 Pixel Recurrent Neural Networks. ICML 2016. [6]. Van den Oord et al. 2016 Conditional Image Generation with PixelCNN Decoders [7] Tim Salimans, Andrej Karpathy, Xi Chen, Diederik P. Kingma PixelCNN++ A PixelCNN Implementation with Discretized Logistic Mixture Likelihood and Other Modifications [8] Germain, Mathieu, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder for distribution estimation ICML 2015. [9] https://towardsdatascience.com/auto-regressive-generative-models-pixelrnn-pixelcnn-32d192911173 .",
            "url": "https://mrtunguyen.github.io//blog/generative/autoregressive/deeplearning/2020/05/20/Autoregressive-Generative-Models.html",
            "relUrl": "/generative/autoregressive/deeplearning/2020/05/20/Autoregressive-Generative-Models.html",
            "date": " • May 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Kaggle Molecular Competition: Lessons Learned from Finishing in Top 5",
            "content": "In the recent Kaggle Predicting Molecular Properties Competition, my team has managed to finish in 5th place (out of 2749 teams). It is the first competition that I spent seriously a lot of time and I learned a lot through it. Though I don’t consider myself as a Kaggle expert by any means, I want to share some lessons, insights that hopefully can be helpful for others. . A little words about Kaggle . Kaggle is the best school, participants are the best teachers and my teammates are the best companions I’ve had . Kaggle is undoubtly a great platform with all sorts of interesting problems along with datasets, kernels and great discussions. Like what I saw in a post long time ago, Kaggle is definitely a home for data science enthusiats all around the world where they spend their days and nights to challenge themselves. And now I would say that I’m very proud to be one of them. Since I’m not graduated from any school formation of data science like many of others, Kaggle comes to me as a place where I can learn many things, keep me motivated in the field which is moving a lot every day. The people like Heng are the best teachers I’ve had, not only from their insights and sharing about competitions but also from the way how they work. Moreover, one of the most important features I love about Kaggle is Leaderboard where we could see where we’re standing compared to others. During this competition, I admit that the first thing I did when waking up every morning is looking at the leaderboard. Seeing many competitors passing above me in the ranking helps me to benchmark my skills, push me to learn and try new things which is very important in my career. So if you want to really get into Machine learning or Data Science, I believe that Kaggle comes in as one of the best ways. . About the competition: Predicting Molecular Properties . This competition is sponsored by the the Chemistry and Mathematics in Phase Space (CHAMPS) at the University of Bristol, Cardiff University, Imperial College and the University of Leeds. It requires competitors to predict the magnetic interaction between two atoms in a molecule (i.e the scalar coupling constant). The objective of using machine learning models in this kind of project is to allow medicinal chemists to gain structural insights faster and cheaper than the quantum mechanic methods and to enable scientists to understand how the 3D chemical structure of a molecule affects its propeties and behavior. Such tools will enable researchers to make progress in a range of important problems, like designing molecules to carry out specific cellular tasks, or designing better drug molecules to fight disease. See more about it in here . Background . At first, my team didn’t had any domain expertise, prior knowledge about chemistry or molecular properties. But we found this competition very interesting to discover a new application of machine learning. Lam is the first who started in our team and realized that using Graph Neural Network (GNN) gains better score than classical machine learning models (like Xgboost or LightGBM). He found out an interesting paper about using GNN for molecular properties and tried to implement it in this competition. After few submissions, he managed to get into top 3 in public Leaderboard at the beginning of the competition. Guillaume and I joined it later with different angle of view. Firstly, I tried to experiment ideas of Heng, using Neural Message Passing for Quantum Chemistry for which he published in the dicussion forum (with his starter code). I also devoted an amount of my time to grasp some knowledge about molecular properties and their interaction. Thereafter, I read different relevant papers in this kind of task in order to better understand how GNN works. The interesting idea behind GNN from machine learning standpoint is finding a representation of a molecule as a graph with atoms as nodes and bonds as edges and how information flow between nodes-nodes, nodes-bonds or bonds-bonds. For example, the assumption that two nodes have a relationship and interact each other is expressed by an edge between them. Similarly, the absence of an edge expresses the assumption that the nodes have no relationship and should not influence each other directly. I also tried another deep learning architecture like Schnet or adding another molecular features by reading dicussions/kernels in kaggle forum before sticking into MEGNET. Lam, Guillaume and I decided to merger as a team one month before competition end when each person has his own good competitive model. For more details, our solution write-up can be found here . And now, below are few lessons that I’ve learned so far from this competition: . 1. A good validation strategy is half of success . I read an interview of bestfitting about his strategy of winning competitions in Kaggle. He said that a good CV is half of success. I couldn’t agree more. Every try we made, it should improve both on our local CV and on the public LB. In this competition, we set aside 5000 moleculars for validation and 80000 ones for training. And luckily, validation score and public leaderboard score is very close so that we are very sure about evaluation of our models. This closeness makes us feel confident with our stacking strategy at the end. So, always trust your CV more than the leaderboard. The public leaderboard represents only 29% of the actual test set, so you can’t be sure about the quality of your solution based on this percentage. Sometimes your model might be great overall, but bad on the data, specially in the public test set. The experience from here and recent finished competition make this lesson more valuable. . 2. Classical Machine learning ideas possibly work in deep learning . When using classical machine learning models like xgboost or Lightgbm, we often heard many times about feature importance technique. While they are widely used in many tabular problems, it is less likely happen in deep learning. But my teammate Guillaume has proven the opposite. After testing feature importance (by randomize one feature at the prediction step), he noticed that the most important feature was by far the angle between an edge and the edge with the closest atom to the first edge. This insight gave us a 0.15 improvement for our best single model. . 3. Always keep updated state-of-the-art of the field (either NLP or Computer Vision) . I encountered many data scientists who said that since they are only working in Computer Vision, they don’t have any interest to invest their time in NLP models. For me, I don’t feel that way. When this competition was finished and the solution of top teams were released, all top 6 teams, except our team, were using a technique that is recently very popular in NLP community - Transformer. The way that they integrated Transformer in their model is quite eye opening for us. . 4. Test ideas with simple model before going bigger . One of the big mistakes I made during this competition is implementing a quite big models in first try. In fact, when my teammates told me about megnet models, I read the paper, write code from scatch and run it with 5 layers while trying to add some new ideas. It took me half day to run and realized that it doesn’t converge at all. Since it is quite deep, I’m kind of stuck in finding why this model doesn’t work as expected. After discussing with my team and simplifing model to only 1 layer, I figured out errors in my implementation. One of the important errors is using Normalization. Indeed, BatchNorm makes the loss fluctuate a lot while LayerNorm works much better in this case. . 5. How to structure code when testing many ideas . One of the problems when working in projects like Kaggle competitions is that how we can make plan of our code so that we can reproduce the results of previous ideas (even idea we tested one month ago) whenever we want. The problem will get bigger and bigger when we’re trying more sophisticated way that make taking notes difficult. The lesson I’ve learned from Heng when looking at his starter code is that create first folder of dataset which contains all dataset, seconde folder which has common functions and a third folder for my principal code. When I want to test new different idea, I will make a copy of the third model and make changes on it. Following this way can make project management easier and I can reproduce my results whenever I need. . 6. Working in a team helps you go faster and further . Besides learning the technical parts of the competition, a very important thing I’ve learned was how to work in a team. We all have work during the week, so we can only do Kaggle at our free time and have to do it in a smart way. Luckily, we worked at the same place, communicated every day and bounced ideas off each other. This competition is my first gold medal! I’m very glad I had this experience and very thankful to be a part of a wonderful team with Lam and Guillaume. I hope we will have more opportunities to work together in future competitions! .",
            "url": "https://mrtunguyen.github.io//blog/kaggle/2019/10/17/Kaggle-Modecular-Competition.html",
            "relUrl": "/kaggle/2019/10/17/Kaggle-Modecular-Competition.html",
            "date": " • Oct 17, 2019"
        }
        
    
  

  
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mrtunguyen.github.io//blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}